<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/2019-tfm-ignacio-arranz/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2019-tfm-ignacio-arranz/" rel="alternate" type="text/html" /><updated>2019-12-28T13:14:33+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Nacho Arranz</name></author><entry><title type="html">Week 19. Testing and exploring the pilot code.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-19/" rel="alternate" type="text/html" title="Week 19. Testing and exploring the pilot code." /><published>2019-12-19T00:00:00+01:00</published><updated>2019-12-19T00:00:00+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-19</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-19/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;New estructure in Neural Behaviors repo.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Execute a neural pilot and pilot solution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;1-new-estructure-in-neural-behaviors-repo&quot;&gt;1. New estructure in Neural Behaviors repo&lt;/h3&gt;

&lt;p&gt;In the process of restructuring the code mentioned in the previous weeks these changes are being taken to the &lt;a href=&quot;https://github.com/JdeRobot/NeuralBehaviors&quot;&gt;Neural Behaviors repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The repository has been subdivided (for the time being) into three lines of research. One, the one already known and in which we are working the last weeks and that part of the final work of Vanessa Fernandez called: “&lt;a href=&quot;https://github.com/JdeRobot/NeuralBehaviors/tree/master/vision-based-end2end-learning&quot;&gt;Vision-based end-to-end using Deep Learning&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;In this repository you can see the structure that is being created to include in it the solutions of &lt;strong&gt;this&lt;/strong&gt; end of master’s work as well as others that may be included.&lt;/p&gt;

&lt;h3 id=&quot;2-execute-a-neural-pilot-and-pilot-solution&quot;&gt;2. Execute a neural pilot and pilot solution&lt;/h3&gt;

&lt;p&gt;Three solutions to the restructured code have been presented here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Manual pilot. Solution taken from the code of the master in artificial vision.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pilot classifier. Using deep learning, values of ‘v’ and ‘w’ distributed in the number of classes corresponding to the loaded model (5 or 7) are returned to the network output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pilot Return: Using also neural networks, it is obtained to the output numerical values that indicate the amount of &lt;code class=&quot;highlighter-rouge&quot;&gt;v&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; that must be applied to the car to follow the line.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the solutions in the following gallery:&lt;/p&gt;

&lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;https://raw.githubusercontent.com/JdeRobot/NeuralBehaviors/master/vision-based-end2end-learning/docs/imgs/piloto_esplicito.gif&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/JdeRobot/NeuralBehaviors/master/vision-based-end2end-learning/docs/imgs/piloto_esplicito.gif&quot; alt=&quot;Manual Pilot&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;https://github.com/JdeRobot/NeuralBehaviors/blob/master/vision-based-end2end-learning/docs/imgs/piloto_neuronal.gif?raw=true&quot;&gt;
        &lt;img src=&quot;https://github.com/JdeRobot/NeuralBehaviors/blob/master/vision-based-end2end-learning/docs/imgs/piloto_neuronal.gif?raw=true&quot; alt=&quot;Classificator Pilot&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;https://github.com/JdeRobot/NeuralBehaviors/blob/master/vision-based-end2end-learning/docs/imgs/piloto_neuronal.gif&quot;&gt;
        &lt;img src=&quot;https://github.com/JdeRobot/NeuralBehaviors/blob/master/vision-based-end2end-learning/docs/imgs/piloto_neuronal.gif&quot; alt=&quot;Regressor Pilot&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Different solutions to the follow-the-line exercise. With and without neural pilot.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;It can be seen as a result that the solution is very similar. For more information visit the Neural &lt;a href=&quot;https://github.com/JdeRobot/NeuralBehaviors&quot;&gt;Behaviors repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;In order to gain confidence and security with neural network training we will train a network from the beginning with an existing dataset. As a result we expect to obtain a solution just like the existing one.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;Seeing the different configurations and possibilities offered by the repository we will focus the project to create a tool that allows switching between different networks and offers quality metrics to compare results. It can be said that what we have learned is that, from something apparently closed as it can be to solve the problem for the final master’s work, we can build a useful tool for future users, students, companies, etc.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="week 19" /><category term="neural_behaviors" /><summary type="html">New structure of Neural Behaviors repository and deeping the Vanessa's code.</summary></entry><entry><title type="html">Week 17-18. Continue with DQN algorithm. Problems with graphic card.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-17-18/" rel="alternate" type="text/html" title="Week 17-18. Continue with DQN algorithm. Problems with graphic card." /><published>2019-12-09T00:00:00+01:00</published><updated>2019-12-09T00:00:00+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-17-18</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-17-18/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Pong game using DQN algorithm.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Train Vanessa’s models.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Take a look at the pilot. Get ideas to implement the based on reinforcement training.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;1-pong-game-using-dqn-algorithm&quot;&gt;1. Pong game using DQN algorithm&lt;/h3&gt;

&lt;p&gt;Temporarily, a solution has been added to the PONG game using Deep Reinforcement Learning but in this case following the guidelines of the Packtl book, which uses PyTorch as a framework.&lt;/p&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1718/dqn_pytorch.gif&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1718/dqn_pytorch.gif&quot; alt=&quot;DQN Solution using PyTorch.&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;The results can be seen in the gif. The training has taken approximately two and a half hours with a team with these characteristics:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/logbook/week1718/dqn_pytorch.gif&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;intel i7 7th generation 2.8 GHz 8-core processor.&lt;/li&gt;
  &lt;li&gt;16 Gb of RAM.&lt;/li&gt;
  &lt;li&gt;Nvidia 1050Ti GPU.&lt;/li&gt;
  &lt;li&gt;256Gb M.2 SSD hard disk.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a &lt;strong&gt;future work&lt;/strong&gt; there is still to solve the same exercise with the TensorFlow framework but &lt;strong&gt;solving the bug&lt;/strong&gt; that appeared in the training.&lt;/p&gt;

&lt;h3 id=&quot;2-train-vanessas-models&quot;&gt;2. Train Vanessa’s models&lt;/h3&gt;

&lt;p&gt;This task has not been solved this week.&lt;/p&gt;

&lt;p&gt;Before the training, a &lt;strong&gt;restructuring of all the code&lt;/strong&gt;, &lt;strong&gt;models&lt;/strong&gt; and &lt;strong&gt;dataset&lt;/strong&gt; has been carried out to adapt it to an order where &lt;strong&gt;more types of similar autonomous driving&lt;/strong&gt; projects fit.&lt;/p&gt;

&lt;h3 id=&quot;3-take-a-look-at-the-pilot-get-ideas-to-implement-the-based-on-reinforcement-training&quot;&gt;3. Take a look at the pilot. Get ideas to implement the based on reinforcement training.&lt;/h3&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1718/TFM-Week_Vanessa_Diagram.png&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1718/TFM-Week_Vanessa_Diagram.png&quot; alt=&quot;Vanessa's master's dissertation diagram.&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Workflow of Vanessa’s master’s dissertation.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;There is still a lot of work to be done in restructuring the initial code. I keep working on it until I have some minimum stability that allows me to face the problem with ‘reinformenet learning’.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;Landing on other people’s code is a difficult job but it forces you to understand how someone else wants their code. Communicating with the other person to reach a common point where the code can grow is an enriching job.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="week 17-18" /><category term="dqn" /><summary type="html">Finishing the DQN algorithm for the game of pong. Exploring the pilot code to implement the equivalent in learning by reinforcement.</summary></entry><entry><title type="html">Week 20. Networking and learning in the Gym-Pyxies tool.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-20/" rel="alternate" type="text/html" title="Week 20. Networking and learning in the Gym-Pyxies tool." /><published>2019-12-05T00:00:00+01:00</published><updated>2019-12-05T00:00:00+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-20</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/landing/refactoring/week-20/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Train a network with the Vanessa’s dataset.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Learning about Gym-Pyxies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;1-train-a-network-with-the-vanessas-dataset&quot;&gt;1. Train a network with the Vanessa’s dataset&lt;/h3&gt;

&lt;h3 id=&quot;2-learning-about-gym-pyxies&quot;&gt;2. Learning about Gym-Pyxies&lt;/h3&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="tutorials" /><category term="week 20" /><category term="neural_behaviors" /><category term="gym pyxies" /><summary type="html">Training of a network given a dataset and learning in the tool Gym-Pyxies for a first approach between Gazebo and OpenAI-gym.</summary></entry><entry><title type="html">Week 15-16. Continue with DQN algorithm. Problems with graphic card.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-15-16/" rel="alternate" type="text/html" title="Week 15-16. Continue with DQN algorithm. Problems with graphic card." /><published>2019-11-17T00:00:00+01:00</published><updated>2019-11-17T00:00:00+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-15-16</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-15-16/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Continuing with the replicate of Vanessa’s master’s degree thesis using Python 3.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Execute and undestanding Pong example from &lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt; repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;1-continuing-with-the-replicate-of-vanessas-masters-degree-thesis-with-python3&quot;&gt;1. Continuing with the replicate of Vanessa’s master’s degree thesis with Python3.&lt;/h3&gt;

&lt;p&gt;During the process of replicating Vanessa Fernandez’s master’s dissertation, the following points are highlighted:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migrating all the code to Python 2 means moving from ROS1 to ROS2. Initially that change is very big to do it in this point of the development of the work so it is set as a future point.&lt;/li&gt;
  &lt;li&gt;In this replication attempt, access to the graphics card was blocked and Gazebo reported an error. I had to reinstall the drivers. Apparently everything is back to normal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-execute-and-undestanding-pong-example-from-puppis-repository&quot;&gt;2. Execute and undestanding Pong example from &lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt; repository.&lt;/h3&gt;

&lt;p&gt;Reading the DQN chapter &lt;a href=&quot;https://books.google.es/books?id=xKdhDwAAQBAJ&amp;amp;lpg=PA141&amp;amp;dq=class%20ExperienceBuffer%3A%20%20%20%20%20def%20__init__(self%2C%20capacity)%3A%20%20%20%20%20%20%20%20%20self.buffer%20%3D%20collections.deque(maxlen%3Dcapacity)%20%20%20%20%20%20def%20__len__(self)%3A%20%20%20%20%20%20%20%20%20return%20len(self.buffer)&amp;amp;hl=es&amp;amp;pg=PA155#v=onepage&amp;amp;q&amp;amp;f=true&quot;&gt;Maxim Lapan - Deep Reinforcement Learning - Hands On&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The file &lt;code class=&quot;highlighter-rouge&quot;&gt;wrappers.py&lt;/code&gt; contains coverage for different situations, such as requiring the player to press START after each repetition.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;dqn_model.py&lt;/code&gt; file contains the network configuration. The model has 3 convolutional layers and 2 fully-connected. All layers are connected by the ReLU activation function.&lt;/p&gt;

&lt;p&gt;Important note from the book:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If transition in the batch is from the lat step in the episode, then out value of the action doesn’t have a discounted reward of the next state, as there is no next state to gather reward from. This main look minor, but this is very important in practice: without this, training will not converge.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;I am currently reading about the DQN algorithm in the previously mentioned book. The goal is to fix the bug in the code in the training of the game of Pong.&lt;/p&gt;

&lt;p&gt;At the same time I’m in contact with Vanessa to create a deployment guide for her algorithm.&lt;/p&gt;

&lt;p&gt;In JdeRobot’s web site a &lt;a href=&quot;https://jderobot.github.io/projects/deep_learning/neural_behavior/&quot;&gt;section&lt;/a&gt; has been created where all the information about &lt;a href=&quot;https://jderobot.github.io/projects/deep_learning/&quot;&gt;DeepLearning projects&lt;/a&gt; that have to do with autonomous driving will be posted.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;Developing the DQN algorithm I realized that I needed to stop trying to solve the problem that occurs in the training to build the same path we take in the study of classical algorithms: &lt;strong&gt;read and study more&lt;/strong&gt;. After that little process, the code was much better understood and made more sense :-)&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="week 15-16" /><category term="dqn" /><summary type="html">Solving problems with the graphic card and reading more information about the DQN algorithm.</summary></entry><entry><title type="html">Week 14. Organization and first algorithm using Deep Learning: DQN.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-14/" rel="alternate" type="text/html" title="Week 14. Organization and first algorithm using Deep Learning: DQN." /><published>2019-11-02T00:00:00+01:00</published><updated>2019-11-02T00:00:00+01:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-14</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-14/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Continuing with the replicate of Vanessa’s master’s degree thesis.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Organize the information of the classical methods.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Try to run some example of OpenAI gym.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;1-continuing-with-the-replicate-of-vanessas-masters-degree-thesis&quot;&gt;1. Continuing with the replicate of Vanessa’s master’s degree thesis.&lt;/h3&gt;

&lt;p&gt;Below is a summary of the steps that have been taken from the first iteration to the present day in replicating Vanessa Fernandez’s master’s dissertation.&lt;/p&gt;

&lt;p&gt;For the replication of Vanessa’s master’s dissertation the packages have been installed:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jderobot
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jderobot-gazebo-assets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The repository has been cloned:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/RoboticsLabURJC/2017-tfm-vanessa-fernandez.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We navigate to the directory where the main program is located:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;2017-tfm-vanessa-fernandez/Follow Line/dl-driver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using two terminals, we launch on one the ROS command that executes the world of Gazebo with:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;roslaunch /opt/jderobot/share/jderobot/launch/f1.launch 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and in another terminal the algorithm is executed:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python driver.py driver.yml 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point I have encountered the following &lt;strong&gt;problems&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the installation of the file &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; the &lt;strong&gt;PyQt5&lt;/strong&gt; library &lt;strong&gt;is not found&lt;/strong&gt;. The installation is trying to be done from inside a Python virtual environment, &lt;strong&gt;it seems that this library is only accessible if you try to install it on the main machine&lt;/strong&gt;. This is a point that should be improved, so the environment of the user’s machine is not affected. &lt;strong&gt;Using Python 3&lt;/strong&gt; seems to solve the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The code is in &lt;strong&gt;Python 2&lt;/strong&gt;, which &lt;strong&gt;will not be maintained&lt;/strong&gt; after 2020. It would be a good idea to try to translate the code to Python 3.5+. With this improvement you would be within a current code execution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Set the &lt;strong&gt;versions of the libraries&lt;/strong&gt; used in &lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt;. Currently the file &lt;code class=&quot;highlighter-rouge&quot;&gt;requiremens.txt&lt;/code&gt; has not fixed any version of the libraries. At the time the end-of-master job was done, the stable TensorFlow version was 1.14. Currently it is version 2.0 so there are some changes that would have to be made to the original code to make it work again. Maybe it would be a good idea to &lt;strong&gt;set&lt;/strong&gt; it to &lt;strong&gt;the&lt;/strong&gt; latest &lt;strong&gt;stable version&lt;/strong&gt;: 1.14.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am currently in contact with Vanessa to solve these small bugs and get some functional models to run the network.&lt;/p&gt;

&lt;h3 id=&quot;2-organize-the-information-of-the-classical-methods&quot;&gt;2. Organize the information of the classical methods.&lt;/h3&gt;

&lt;p&gt;So far the following classical methods have been studied:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QLearning.&lt;/li&gt;
  &lt;li&gt;SARSA.&lt;/li&gt;
  &lt;li&gt;Dynamic Programming (Policy and Value iteration).&lt;/li&gt;
  &lt;li&gt;Monte Carlo.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The division between the way they face the result can be seen in the following table.&lt;/p&gt;

&lt;style type=&quot;text/css&quot;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
.tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
&lt;/style&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-9wq8&quot; rowspan=&quot;4&quot;&gt;Model-free methods&lt;/th&gt;
    &lt;th class=&quot;tg-lboi&quot; rowspan=&quot;2&quot;&gt;Value-based methods&lt;/th&gt;
    &lt;th class=&quot;tg-lboi&quot; rowspan=&quot;2&quot;&gt;Temporal Difference (TD)&lt;/th&gt;
    &lt;th class=&quot;tg-lboi&quot;&gt;Q-Learning&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-9wq8&quot;&gt;SARSA&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot; rowspan=&quot;2&quot;&gt;Policy-based methods&lt;br /&gt;&lt;/td&gt;
    &lt;td class=&quot;tg-9wq8&quot; colspan=&quot;2&quot;&gt;Policy Iteration&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-9wq8&quot; colspan=&quot;2&quot;&gt;Value Iteration&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-9wq8&quot;&gt;Model-based methods&lt;/td&gt;
    &lt;td class=&quot;tg-9wq8&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-9wq8&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-9wq8&quot;&gt;-&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Soon I will include more information on each of the algorithms.&lt;/p&gt;

&lt;h3 id=&quot;3-run-an-example-of-dqn-algorithms&quot;&gt;3. Run an example of DQN algorithms.&lt;/h3&gt;

&lt;p&gt;The example exercise that I have chosen to train and execute a DQN algorithm is the example of Alberto Martin’s repository with the PONG game. The goal is, once this one works, to try to replicate the same code with the Space Invaders game (I really like that game :-) ).&lt;/p&gt;

&lt;p&gt;Although the original code was configured using TensorFlow 2.0, this version was in alpha phase. Installing directly the &lt;code class=&quot;highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; libraries does not execute the algorithm correctly. Creating a virtual environment and installing the libraries, the result of the training returns the following error:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_file_writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'runs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flush_millis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename_suffix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-dqn-pong&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tensorflow'&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attribute&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'contrib'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As I said before, since the version with which this exercise was configured was in alpha, the command that gives the error is not in the set of instructions. According to &lt;a href=&quot;https://stackoverflow.com/questions/55870127/module-tensorflow-has-no-attribute-contrib&quot;&gt;this thread of StackOverflow&lt;/a&gt; it is recommended to execute the script that gives the official documentation.&lt;/p&gt;

&lt;p&gt;This process has been done and created a parallel folder with the same files but with the translated commands.&lt;/p&gt;

&lt;p&gt;Running the file &lt;code class=&quot;highlighter-rouge&quot;&gt;dqn_pong.py&lt;/code&gt; again we have the following error whe the algoritm is training (in the step 11):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; 
&lt;span class=&quot;mi&quot;&gt;435&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;games&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.625&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.93&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;762.74&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;8253&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;games&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.667&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.92&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;732.73&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;9106&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;games&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.700&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.91&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;708.14&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;9967&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;games&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.727&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;732.39&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Traceback&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;call&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;File&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dqn_pong.py&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;172&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tgt_net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;File&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/path/to/virtualenvironment/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1327&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set_weights&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'...'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;You&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;called&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`set_weights(weights)`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dqn_1&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;but&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;was&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expecting&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Provided&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[[[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.71510142e-02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.64676267e-02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.32&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;I am currently working in parallel on the three previous points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Organizing the documentation of what has been learned from the classical algorithms as well as deepening the operation.&lt;/li&gt;
  &lt;li&gt;Finalizing the errors and improving the code structure of Vanessa’s end-of-master work.&lt;/li&gt;
  &lt;li&gt;Solving the bugs in the execution of the given Pong in the repository (folder &lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="week 14" /><category term="dqn" /><summary type="html">Execution of the Pong game from the code provided in Alberto's repository.</summary></entry><entry><title type="html">Week 13. Installing nvidia-docker and undestanding DRL algorithms. Part (II).</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-13/" rel="alternate" type="text/html" title="Week 13. Installing nvidia-docker and undestanding DRL algorithms. Part (II)." /><published>2019-10-21T00:00:00+02:00</published><updated>2019-10-21T00:00:00+02:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-13</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-13/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Studying new classical algorithms.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Installing OpenCV using Qt (instead of GTK).&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Continuing with the replicate of Vanessa’s master’s degree thesis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;h3 id=&quot;studying-new-classical-algorithms&quot;&gt;Studying new classical algorithms.&lt;/h3&gt;

&lt;p&gt;During the week I have selected two classic Reinforcement Learning algorithms in which to focus the study and deepen the operation. These have been: Dynamic Programming and Monte Carlo.&lt;/p&gt;

&lt;p&gt;To understand how they work, I had as a reference book: “&lt;a href=&quot;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&quot;&gt;Reinforcement Learning:
An Introduction&lt;/a&gt;” by Richard S. Sutton and Andrew G. Barto.&lt;/p&gt;

&lt;p&gt;In addition, I have been supporting the reading with other articles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919&quot;&gt;Reinforcement Learning Demystified: Solving MDPs with Dynamic Programming&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4&quot;&gt;Model Free Reinforcement learning algorithms (Monte Carlo, SARSA, Q-learning)&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511&quot;&gt;Monte Carlo in Reinforcement Learning, the Easy Way&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;installing-opencv-using-qt-instead-of-gtk&quot;&gt;Installing OpenCV using Qt (instead of GTK).&lt;/h3&gt;

&lt;p&gt;I have tried &lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/&quot;&gt;compile OpenCV&lt;/a&gt; on the computer using &lt;em&gt;flag&lt;/em&gt; to create using PyQT instead of GTK since in the Ubuntu version I use (Kubuntu: Ubuntu + KDE Plasma) sometimes has problems to raise a window.&lt;/p&gt;

&lt;p&gt;In the process of trying to compile it I found the following error:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Makefile:162: recipe for target 'all' failed 
make: *** [all] Error 2 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are still working to find a way to compile the program.&lt;/p&gt;

&lt;p&gt;[UPDATE] - Finally, i’m working with Ubuntu Operating System. The problem was solved.&lt;/p&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;I will continue to try to replicate Vanessa’s master’s dissertation to try to close that road.&lt;/p&gt;

&lt;p&gt;On the other hand, continue studying classic Reinforcement Learning methods and try the first Deep Reinforcement Learning algorithm in Alberto’s repository, &lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt; directory. It would be to solve the game of Pong using the DQN algorithm.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;Knowing the different classical methods gives an idea of ćomo work before different problems to find the best option. Although this week I have known better these algorithms, I think that to continue studying them is the way to settle and assimilate all the concepts.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="week 13" /><category term="docker" /><category term="exercises" /><summary type="html">Studying the methods seen the previous week (continuing).</summary></entry><entry><title type="html">Week 12. Installing nvidia-docker and undestanding DRL algorithms.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-12/" rel="alternate" type="text/html" title="Week 12. Installing nvidia-docker and undestanding DRL algorithms." /><published>2019-10-16T00:00:00+02:00</published><updated>2019-10-16T00:00:00+02:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-12</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-12/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Do different exercises and study Deep Reinforcement Learning algorithms.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Replicating Vanessa Fernandez’s master’s dissertation. Trying to pack everything in a single Docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;p&gt;I have tried to replicate Vanessa Martinez’s end-of-master work inside a Docker container to try to isolate all the components and have them located in a few files.&lt;/p&gt;

&lt;p&gt;For this process it is necessary to uninstall &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.io&lt;/code&gt; (it is the one that is installed by default in a normal docker installation) and install &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-engine&lt;/code&gt; where plugins can be installed to enrich the program. &lt;em&gt;Important&lt;/em&gt;: Is required to have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-engine &amp;gt;= 19.02&lt;/code&gt;. It’s recommended to uninstall previous versions of Docker by following &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Among the necessary plugins is &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-docker&lt;/code&gt;, which cloning the official image has the necessary libraries to obtain the resources of the graphics card. &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-docker&lt;/code&gt; image can be downloaded in &lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The other point of the week was to study some of the classic algorithms to strengthen the knowledge acquired in previous weeks and see the applications in specific algorithms.&lt;/p&gt;

&lt;p&gt;The two methods studied this week are two classic time-difference algorithms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QLearning&lt;/li&gt;
  &lt;li&gt;SARSA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main difference between the two methods is that they differ in policy monitoring. The first one does not follow a policy and the main objective is to seek the value that maximizes the Q function regardless of how it has achieved it. The second of them follows a policy and maximising its value is linked to the way it learns.&lt;/p&gt;

&lt;p&gt;I will add more information in the coming days.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e&quot;&gt;Reinforcement learning: Temporal-Difference, SARSA, Q-Learning &amp;amp; Expected SARSA in python&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789345803/1/ch01lvl1sec13/sarsa-versus-q-learning-on-policy-or-off&quot;&gt;SARSA versus Q-learning – on-policy or off?&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce&quot;&gt;TD in Reinforcement Learning, the Easy Way&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lipn.univ-paris13.fr/~gerard/docs/publications/rodrigues-ger-rou-ilp08-submit.pdf&quot;&gt;On and Off-Policy Relational Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;Given the size that the Docker image can have after installing all the dependencies as well as the fact that the exercise presented by Vanessa Fernandez requires a graphic interface, we will try a mixed configuration where everything that depends on the graphic card will be grouped inside the Docker container.&lt;/p&gt;

&lt;p&gt;The Ubuntu I work with is version 18.04 with the KDE desktop interface. To prevent collisions between libraries in the operating system and OpenCV I will try to compile and install the library with the Qt flag instead of the default GTK flag.&lt;/p&gt;

&lt;p&gt;In the process of learning and familiarisation with the reinforcement learning algorithms, the following will continue to be studied within the classical algorithms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cross entropy&lt;/li&gt;
  &lt;li&gt;Dynamic programming.&lt;/li&gt;
  &lt;li&gt;Monte Carlo.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;This week I have focused mainly on getting to know more deeply the differences between various methods of LR. In particular, everything related to Q-Learning and SARSA. Their differences between politics, advantages and disadvantages, usefulness, etc. Knowing in a deeper way classical RL algorithms like the ones mentioned above, helps to understand the way in which DRL algorithms are taken further. I am currently studying different classical algorithms such as Dynamic Programming or Monte Carlo.&lt;/p&gt;

&lt;p&gt;Continue …&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="studying" /><category term="tutorials" /><category term="weeks 12" /><category term="docker" /><category term="exercises" /><summary type="html">Studying the methods seen the previous week.</summary></entry><entry><title type="html">Week 10-11. Running different exercises from repo</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-10-11/" rel="alternate" type="text/html" title="Week 10-11. Running different exercises from repo" /><published>2019-10-12T00:00:00+02:00</published><updated>2019-10-12T00:00:00+02:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-10-11</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-10-11/">&lt;p&gt;These weeks have been for the study and understanding of the different methods and agents that exist in Alberto Martín’s repository. This brings us closer to the real environments where the work will be developed.&lt;/p&gt;

&lt;h1 id=&quot;to-do&quot;&gt;To Do&lt;/h1&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Perform different exercises from the “puppis” section. &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/puppis&quot;&gt;repository&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;(OnGoing)Install environment to replicate examples of &lt;a href=&quot;https://github.com/RoboticsLabURJC/2017-tfm-vanessa-fernandez&quot;&gt;Vanessa Fernandez’s&lt;/a&gt; end-of-master job.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;progress&quot;&gt;Progress&lt;/h1&gt;

&lt;p&gt;In this period different exercises of the Gym environment have been executed where the different learning methods are tested before different agents.&lt;/p&gt;

&lt;p&gt;The tested methods have been:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cross_entropy&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dqn&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dynamic_programming&lt;/code&gt; (with policy and value iteration).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;monte_carlo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;temporal_difference&lt;/code&gt; (with and without policy). With QLearning and Sarsa algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the execution of the methods in the ‘CartPole’ scenario you have to launch the instruction from the previous directory to ‘&lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt;’:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For the SARSA method:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; agents.gym.cartpole.sarsa_agent 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the QLearning method:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; agents.gym.cartpole.q_learning_agent
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These commands put to execute the training of the environment ‘CartPole’ where it can be seen that when a threshold of inclination is exceeded the exercise is restarted and the learned values are updated to apply it on the next epoch (if it improves with respect to the previous one).&lt;/p&gt;

&lt;p&gt;The solution to the cross_entropy method can be seen in the gif:&lt;/p&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1011/cartpole_solution.gif&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week1011/cartpole_solution.gif&quot; alt=&quot;Image week 10-11&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;cross_entropy solution.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;I’m currently reviewing the gym ‘pong’ environment training to get a performance where the agent learns to play the video game.&lt;/p&gt;

&lt;p&gt;In addition, I am in communication with Vanessa Fernández to replicate the environment of her end-of-master work.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;At the Python programming level, I have learned to launch programs with modules in parallel using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-m&lt;/code&gt; argument. With this it is possible to have the infrastructure separated in a logical way to apply different agents to different methods.&lt;/p&gt;

&lt;p&gt;As for the exercises in the repository, there is a clear difference in the times of training and final solution. The &lt;code class=&quot;highlighter-rouge&quot;&gt;cross_entropy&lt;/code&gt; method has been the fastest to solve in training with very good results in execution.&lt;/p&gt;

&lt;p&gt;Methods such as Q-Learning are less effective at a time when there is no clear separation of all environmental states.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorials" /><category term="weeks 10-11" /><summary type="html">Running different methods and agents for the resolution of DRL problems.</summary></entry><entry><title type="html">Week 9. Landing on the working infrastructure</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-9/" rel="alternate" type="text/html" title="Week 9. Landing on the working infrastructure" /><published>2019-09-23T00:00:00+02:00</published><updated>2019-09-23T00:00:00+02:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-9</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/examples/landing/week-9/">&lt;p&gt;This week we are going to install the necessary libraries to have the TensorFlow library installed as well as some examples from &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;to-do&quot;&gt;To Do&lt;/h1&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Install the Dockerfile &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/dockers&quot;&gt;provided here&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Replicate the infrastructure of the mentioned &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin&quot;&gt;repository&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Install environment to replicate examples of &lt;a href=&quot;https://github.com/RoboticsLabURJC/2017-tfm-vanessa-fernandez&quot;&gt;Vanessa Fernandez’s&lt;/a&gt; end-of-master job.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;progress&quot;&gt;Progress&lt;/h1&gt;

&lt;p&gt;Some of the robotics libraries that will be used during the development of this work are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;jderobot.org&quot;&gt;JdeRobot&lt;/a&gt; (open toolkit for developing Robotics applications).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gazebosim.org&quot;&gt;Gazebo&lt;/a&gt; (simulator).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ros.org&quot;&gt;ROS&lt;/a&gt; (Robot Operating System).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these tools we will have a part of the infrastructure where the learning algorithms will be trained later by “Reinforcement Learning”.&lt;/p&gt;

&lt;h2 id=&quot;install-docker-images&quot;&gt;Install Docker images.&lt;/h2&gt;

&lt;p&gt;To install the docker you need the Dockerfile files from &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/dockers&quot;&gt;this repository&lt;/a&gt; with the command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; cuda_cudnn_ros_melodic_python3 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; ros_melodic_gazebo &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;replicate-examples-from-the-repository&quot;&gt;Replicate examples from the repository&lt;/h2&gt;

&lt;p&gt;As a first objective I wanted to launch a test world of Gazebo inside the container as well as the GzWeb client to be able to visualize it in the web browser (outside the container).&lt;/p&gt;

&lt;p&gt;To do this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Start the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-ti&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 11311:11311 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 11345:11345 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 7681:7681 ros_melodic_gazebo bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following instructions inside the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;roscore &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We clone and run a test world of the repository:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/RoboticsLabURJC/2019-phd-alberto-martin
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;2019-phd-alberto-martin/gym-pyxis/gym_pyxis/envs/gazebo/assets/launch
roslaunch turtlebot3_followline.launch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another terminal (for simplicity) we run the Gazebo (web) client inside the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;gzweb
npm start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result is that the world can be seen in the browser as in the images:&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week9/week-9_image1.png&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week9/week-9_image1.png&quot; alt=&quot;Image week 9 - 1&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week9/week-9_image2.png&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week9/week-9_image2.png&quot; alt=&quot;Image week 9 - 2&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Samples of Gazebo Worlds launched by ROS inside the Docker.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;I’m working on replicating the Vanessa Martinez’s end-of-master work environment.&lt;/li&gt;
  &lt;li&gt;Reading &lt;a href=&quot;https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296&quot;&gt;Advantage Actor Critic (A3C) article&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;First approach to the environment where the work will be developed.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorials" /><category term="weeks 9" /><summary type="html">Doing some examples and installing infrastructure.</summary></entry><entry><title type="html">Week 7-8. Reading more documentation and small tutorials.</title><link href="http://localhost:4000/2019-tfm-ignacio-arranz/previous%20work/week-7-8/" rel="alternate" type="text/html" title="Week 7-8. Reading more documentation and small tutorials." /><published>2019-09-18T00:00:00+02:00</published><updated>2019-09-18T00:00:00+02:00</updated><id>http://localhost:4000/2019-tfm-ignacio-arranz/previous%20work/week-7-8</id><content type="html" xml:base="http://localhost:4000/2019-tfm-ignacio-arranz/previous%20work/week-7-8/">&lt;p&gt;In these two weeks, given the amount of information I found, I decided to put into practice some simple tutorial to help me settle all the concepts and be able to relate them.&lt;/p&gt;

&lt;h2 id=&quot;small-tutorial&quot;&gt;Small tutorial&lt;/h2&gt;

&lt;p&gt;The website that helps me is the one provided by &lt;a href=&quot;https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/&quot;&gt;learndatasci&lt;/a&gt; with a guided exercise for the taxi game.&lt;/p&gt;

&lt;p&gt;To see if all the steps were correct, I have been replicating each step with the concepts in a Jupyter booklet that I have published in &lt;a href=&quot;https://github.com/igarag/machine_learning/blob/master/reinforcement-learning-projects/taxi/DRL_test.ipynb&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This short tutorial explains how to make your first Reinforcement Learning algorithm using Q-Learning for a taxi (represented by a yellow cursor) to pick up and deliver passengers from specific locations. The goal is for it to do so through the shortest route by correctly picking up passengers at their stop and delivering them to the stop.&lt;/p&gt;

&lt;p&gt;This algorithm has two tables, $Q_{table}$ and within it $Q_{values}$. The latter will be updated to remember the most beneficial action in the previous stage. The better $Q_{values}$ will have better opportunities to have better rewards. The $Q_{values}$ (arbitrarily initialized) will be updated using the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\prime}_{(state, action)} = (1 - \alpha) Q_{(s_t,a_t)} + \alpha [r(s_t, a_t) +\gamma \max_{\alpha}Q_{(s_{t+1},a_{t+1})}]&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$r$: is the reward.&lt;/li&gt;
  &lt;li&gt;$\alpha$: is the learning rate ($0&amp;lt; \alpha \leq 1$)&lt;/li&gt;
  &lt;li&gt;$\gamma$: Discount factor ($o &amp;lt; \gamma \leq 1$). Determine how important we want to be in future rewards. A high value (about 1) considers long term rewards and about 0 makes the agent consider the immediate recommendation, i.e. greedy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learned value is a combination of the rewards for taking the current value in the current state and the discounted maximum rewards of the next state we will be in once we take the current action.&lt;/p&gt;

&lt;p&gt;To prevent the algorithm from learning a “fixed path” (overfitting) an Epsilon parameter ($\epsilon$) is introduced. It combines two states: the exploration and exploitation dilemma.&lt;/p&gt;

&lt;p&gt;A low epsilon will occur in episodes with more penalties. This will be the case when the algorithm is exploring, since the parameter $\epsilon$ does not influence the agent. The result is as follows:&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week78/tutorial-q-learning.gif&quot; title=&quot;Without QLearning&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week78/tutorial-q-learning.gif&quot; alt=&quot;Without QLearning&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week78/tutorial-q-learning_solution.gif&quot; title=&quot;Solution using QLearning.&quot;&gt;
        &lt;img src=&quot;/2019-tfm-ignacio-arranz/assets/images/logbook/week78/tutorial-q-learning_solution.gif&quot; alt=&quot;Solution using QLearning.&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Taxi solution. Left with NO QLearning. Right with QLearning algorithm.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;With this tutorial I understood how the $\epsilon$ parameter works in this field. The algorithm would act in certain cases just like humans. If something has gone well for me, why try different combinations? With this parameter the algorithm is being forced to explore other paths or to exploit the best path.&lt;/p&gt;

&lt;p&gt;For more information visit the original article or the Jupyter notebook mentioned above.&lt;/p&gt;

&lt;h2 id=&quot;items-read&quot;&gt;Items read.&lt;/h2&gt;

&lt;p&gt;After this, I try to read other papers related to other types of Reinforcement Learning. These papers are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://karpathy.github.io/2016/05/31/rl/&quot;&gt;&lt;strong&gt;Pong from pixels&lt;/strong&gt;&lt;/a&gt;. The tutorial has been followed and analyzed in this &lt;a href=&quot;https://github.com/igarag/machine_learning/blob/master/reinforcement-learning-projects/pong/QLearning/pong_notebook.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296&quot;&gt;&lt;strong&gt;Playing CartPole through Asynchronous Advantage Actor Critic (A3C)&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://hackernoon.com/google-xs-deep-reinforcement-learning-in-robotics-using-vision-7a78e87ab171&quot;&gt;&lt;strong&gt;Google X’s Deep Reinforcement Learning in Robotics using Vision&lt;/strong&gt;&lt;/a&gt;. Presents the &lt;a href=&quot;https://arxiv.org/abs/1806.10293&quot;&gt;Qt-Opt algorithm&lt;/a&gt; is designed by combining two methods:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Large-scale Distributed Optimization&lt;/strong&gt;. Using multiple robots to train model faster, making it a large-scale distributed system.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Deep Q-learning algorithm&lt;/strong&gt;. RL technique used to learn a policy, which tells an agent which action to take under which circumstances.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorial" /><category term="previous work" /><category term="weeks 7-8" /><summary type="html">Apply what you have learned to a small tutorial</summary></entry></feed>