<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-23T22:44:11+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Nacho Arranz</name></author><entry><title type="html">Week 12. Installing nvidia-docker and undestanding DRL algorithms.</title><link href="http://localhost:4000/examples/landing/week-12/" rel="alternate" type="text/html" title="Week 12. Installing nvidia-docker and undestanding DRL algorithms." /><published>2019-10-16T00:00:00+02:00</published><updated>2019-10-16T00:00:00+02:00</updated><id>http://localhost:4000/examples/landing/week-12</id><content type="html" xml:base="http://localhost:4000/examples/landing/week-12/">&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Do different exercises and study Deep Reinforcement Learning algorithms.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Replicating Vanessa Fernandez’s master’s dissertation. Trying to pack everything in a single Docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;progress&quot;&gt;Progress&lt;/h2&gt;

&lt;p&gt;I have tried to replicate Vanessa Martinez’s end-of-master work inside a Docker container to try to isolate all the components and have them located in a few files.&lt;/p&gt;

&lt;p&gt;For this process it is necessary to uninstall &lt;code class=&quot;highlighter-rouge&quot;&gt;docker.io&lt;/code&gt; (it is the one that is installed by default in a normal docker installation) and install &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-engine&lt;/code&gt; where plugins can be installed to enrich the program. &lt;em&gt;Important&lt;/em&gt;: Is required to have &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-engine &amp;gt;= 19.02&lt;/code&gt;. It’s recommended to uninstall previous versions of Docker by following &lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/ubuntu/&quot;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Among the necessary plugins is &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-docker&lt;/code&gt;, which cloning the official image has the necessary libraries to obtain the resources of the graphics card. &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-docker&lt;/code&gt; image can be downloaded in &lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The other point of the week was to study some of the classic algorithms to strengthen the knowledge acquired in previous weeks and see the applications in specific algorithms.&lt;/p&gt;

&lt;p&gt;The two methods studied this week are two classic time-difference algorithms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QLearning&lt;/li&gt;
  &lt;li&gt;SARSA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main difference between the two methods is that they differ in policy monitoring. The first one does not follow a policy and the main objective is to seek the value that maximizes the Q function regardless of how it has achieved it. The second of them follows a policy and maximising its value is linked to the way it learns.&lt;/p&gt;

&lt;p&gt;I will add more information in the coming days.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e&quot;&gt;Reinforcement learning: Temporal-Difference, SARSA, Q-Learning &amp;amp; Expected SARSA in python&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789345803/1/ch01lvl1sec13/sarsa-versus-q-learning-on-policy-or-off&quot;&gt;SARSA versus Q-learning – on-policy or off?&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce&quot;&gt;TD in Reinforcement Learning, the Easy Way&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lipn.univ-paris13.fr/~gerard/docs/publications/rodrigues-ger-rou-ilp08-submit.pdf&quot;&gt;On and Off-Policy Relational Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;Given the size that the Docker image can have after installing all the dependencies as well as the fact that the exercise presented by Vanessa Fernandez requires a graphic interface, we will try a mixed configuration where everything that depends on the graphic card will be grouped inside the Docker container.&lt;/p&gt;

&lt;p&gt;The Ubuntu I work with is version 18.04 with the KDE desktop interface. To prevent collisions between libraries in the operating system and OpenCV I will try to compile and install the library with the Qt flag instead of the default GTK flag.&lt;/p&gt;

&lt;p&gt;In the process of learning and familiarisation with the reinforcement learning algorithms, the following will continue to be studied within the classical algorithms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cross entropy&lt;/li&gt;
  &lt;li&gt;Dynamic programming.&lt;/li&gt;
  &lt;li&gt;Monte Carlo.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;This week I have focused mainly on getting to know more deeply the differences between various methods of LR. In particular, everything related to Q-Learning and SARSA. Their differences between politics, advantages and disadvantages, usefulness, etc. Knowing in a deeper way classical RL algorithms like the ones mentioned above, helps to understand the way in which DRL algorithms are taken further. I am currently studying different classical algorithms such as Action-critic (A3C).&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorials" /><category term="weeks 12" /><category term="docker" /><category term="exercises" /><summary type="html">Studying the methods seen the previous week.</summary></entry><entry><title type="html">Week 10-11. Running different exercises from repo</title><link href="http://localhost:4000/examples/landing/week-10-11/" rel="alternate" type="text/html" title="Week 10-11. Running different exercises from repo" /><published>2019-10-12T00:00:00+02:00</published><updated>2019-10-12T00:00:00+02:00</updated><id>http://localhost:4000/examples/landing/week-10-11</id><content type="html" xml:base="http://localhost:4000/examples/landing/week-10-11/">&lt;p&gt;These weeks have been for the study and understanding of the different methods and agents that exist in Alberto Martín’s repository. This brings us closer to the real environments where the work will be developed.&lt;/p&gt;

&lt;h1 id=&quot;to-do&quot;&gt;To Do&lt;/h1&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Perform different exercises from the “puppis” section. &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/puppis&quot;&gt;repository&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;(OnGoing)Install environment to replicate examples of &lt;a href=&quot;https://github.com/RoboticsLabURJC/2017-tfm-vanessa-fernandez&quot;&gt;Vanessa Fernandez’s&lt;/a&gt; end-of-master job.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;progress&quot;&gt;Progress&lt;/h1&gt;

&lt;p&gt;In this period different exercises of the Gym environment have been executed where the different learning methods are tested before different agents.&lt;/p&gt;

&lt;p&gt;The tested methods have been:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cross_entropy&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dqn&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dynamic_programming&lt;/code&gt; (with policy and value iteration).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;monte_carlo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;temporal_difference&lt;/code&gt; (with and without policy). With QLearning and Sarsa algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the execution of the methods in the ‘CartPole’ scenario you have to launch the instruction from the previous directory to ‘&lt;code class=&quot;highlighter-rouge&quot;&gt;puppis&lt;/code&gt;’:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For the SARSA method:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; agents.gym.cartpole.sarsa_agent 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the QLearning method:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; agents.gym.cartpole.q_learning_agent
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These commands put to execute the training of the environment ‘CartPole’ where it can be seen that when a threshold of inclination is exceeded the exercise is restarted and the learned values are updated to apply it on the next epoch (if it improves with respect to the previous one).&lt;/p&gt;

&lt;p&gt;The solution to the cross_entropy method can be seen in the gif:&lt;/p&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;a href=&quot;/assets/images/logbook/week1011/cartpole_solution.gif&quot;&gt;
        &lt;img src=&quot;/assets/images/logbook/week1011/cartpole_solution.gif&quot; alt=&quot;Image week 10-11&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;cross_entropy solution.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;p&gt;I’m currently reviewing the gym ‘pong’ environment training to get a performance where the agent learns to play the video game.&lt;/p&gt;

&lt;p&gt;In addition, I am in communication with Vanessa Fernández to replicate the environment of her end-of-master work.&lt;/p&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;At the Python programming level, I have learned to launch programs with modules in parallel using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-m&lt;/code&gt; argument. With this it is possible to have the infrastructure separated in a logical way to apply different agents to different methods.&lt;/p&gt;

&lt;p&gt;As for the exercises in the repository, there is a clear difference in the times of training and final solution. The &lt;code class=&quot;highlighter-rouge&quot;&gt;cross_entropy&lt;/code&gt; method has been the fastest to solve in training with very good results in execution.&lt;/p&gt;

&lt;p&gt;Methods such as Q-Learning are less effective at a time when there is no clear separation of all environmental states.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorials" /><category term="weeks 10-11" /><summary type="html">Running different methods and agents for the resolution of DRL problems.</summary></entry><entry><title type="html">Week 9. Landing on the working infrastructure</title><link href="http://localhost:4000/examples/landing/week-9/" rel="alternate" type="text/html" title="Week 9. Landing on the working infrastructure" /><published>2019-09-23T00:00:00+02:00</published><updated>2019-09-23T00:00:00+02:00</updated><id>http://localhost:4000/examples/landing/week-9</id><content type="html" xml:base="http://localhost:4000/examples/landing/week-9/">&lt;p&gt;This week we are going to install the necessary libraries to have the TensorFlow library installed as well as some examples from &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;to-do&quot;&gt;To Do&lt;/h1&gt;

&lt;p&gt;The tasks proposed for this week are&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Install the Dockerfile &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/dockers&quot;&gt;provided here&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Replicate the infrastructure of the mentioned &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin&quot;&gt;repository&lt;/a&gt;.&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Install environment to replicate examples of &lt;a href=&quot;https://github.com/RoboticsLabURJC/2017-tfm-vanessa-fernandez&quot;&gt;Vanessa Fernandez’s&lt;/a&gt; end-of-master job.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;progress&quot;&gt;Progress&lt;/h1&gt;

&lt;p&gt;Some of the robotics libraries that will be used during the development of this work are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;jderobot.org&quot;&gt;JdeRobot&lt;/a&gt; (open toolkit for developing Robotics applications).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gazebosim.org&quot;&gt;Gazebo&lt;/a&gt; (simulator).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ros.org&quot;&gt;ROS&lt;/a&gt; (Robot Operating System).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these tools we will have a part of the infrastructure where the learning algorithms will be trained later by “Reinforcement Learning”.&lt;/p&gt;

&lt;h2 id=&quot;install-docker-images&quot;&gt;Install Docker images.&lt;/h2&gt;

&lt;p&gt;To install the docker you need the Dockerfile files from &lt;a href=&quot;https://github.com/RoboticsLabURJC/2019-phd-alberto-martin/tree/master/dockers&quot;&gt;this repository&lt;/a&gt; with the command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; cuda_cudnn_ros_melodic_python3 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; ros_melodic_gazebo &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;replicate-examples-from-the-repository&quot;&gt;Replicate examples from the repository&lt;/h2&gt;

&lt;p&gt;As a first objective I wanted to launch a test world of Gazebo inside the container as well as the GzWeb client to be able to visualize it in the web browser (outside the container).&lt;/p&gt;

&lt;p&gt;To do this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Start the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-ti&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 11311:11311 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 11345:11345 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 7681:7681 ros_melodic_gazebo bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following instructions inside the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;roscore &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We clone and run a test world of the repository:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/RoboticsLabURJC/2019-phd-alberto-martin
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;2019-phd-alberto-martin/gym-pyxis/gym_pyxis/envs/gazebo/assets/launch
roslaunch turtlebot3_followline.launch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another terminal (for simplicity) we run the Gazebo (web) client inside the container:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;gzweb
npm start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result is that the world can be seen in the browser as in the images:&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;a href=&quot;/assets/images/logbook/week9/week-9_image1.png&quot;&gt;
        &lt;img src=&quot;/assets/images/logbook/week9/week-9_image1.png&quot; alt=&quot;Image week 9 - 1&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/logbook/week9/week-9_image2.png&quot;&gt;
        &lt;img src=&quot;/assets/images/logbook/week9/week-9_image2.png&quot; alt=&quot;Image week 9 - 2&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Samples of Gazebo Worlds launched by ROS inside the Docker.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;working&quot;&gt;Working&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;I’m working on replicating the Vanessa Martinez’s end-of-master work environment.&lt;/li&gt;
  &lt;li&gt;Reading &lt;a href=&quot;https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296&quot;&gt;Advantage Actor Critic (A3C) article&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learned&quot;&gt;Learned&lt;/h2&gt;

&lt;p&gt;First approach to the environment where the work will be developed.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorials" /><category term="weeks 9" /><summary type="html">Doing some examples and installing infrastructure.</summary></entry><entry><title type="html">Week 7-8. Reading more documentation and small tutorials.</title><link href="http://localhost:4000/previous%20work/week-7-8/" rel="alternate" type="text/html" title="Week 7-8. Reading more documentation and small tutorials." /><published>2019-09-18T00:00:00+02:00</published><updated>2019-09-18T00:00:00+02:00</updated><id>http://localhost:4000/previous%20work/week-7-8</id><content type="html" xml:base="http://localhost:4000/previous%20work/week-7-8/">&lt;p&gt;In these two weeks, given the amount of information I found, I decided to put into practice some simple tutorial to help me settle all the concepts and be able to relate them.&lt;/p&gt;

&lt;h2 id=&quot;small-tutorial&quot;&gt;Small tutorial&lt;/h2&gt;

&lt;p&gt;The website that helps me is the one provided by &lt;a href=&quot;https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/&quot;&gt;learndatasci&lt;/a&gt; with a guided exercise for the taxi game.&lt;/p&gt;

&lt;p&gt;To see if all the steps were correct, I have been replicating each step with the concepts in a Jupyter booklet that I have published in &lt;a href=&quot;https://github.com/igarag/machine_learning/blob/master/reinforcement-learning-projects/taxi/DRL_test.ipynb&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This short tutorial explains how to make your first Reinforcement Learning algorithm using Q-Learning for a taxi (represented by a yellow cursor) to pick up and deliver passengers from specific locations. The goal is for it to do so through the shortest route by correctly picking up passengers at their stop and delivering them to the stop.&lt;/p&gt;

&lt;p&gt;This algorithm has two tables, $Q_{table}$ and within it $Q_{values}$. The latter will be updated to remember the most beneficial action in the previous stage. The better $Q_{values}$ will have better opportunities to have better rewards. The $Q_{values}$ (arbitrarily initialized) will be updated using the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\prime}_{(state, action)} = (1 - \alpha) Q_{(s_t,a_t)} + \alpha [r(s_t, a_t) +\gamma \max_{\alpha}Q_{(s_{t+1},a_{t+1})}]&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$r$: is the reward.&lt;/li&gt;
  &lt;li&gt;$\alpha$: is the learning rate ($0&amp;lt; \alpha \leq 1$)&lt;/li&gt;
  &lt;li&gt;$\gamma$: Discount factor ($o &amp;lt; \gamma \leq 1$). Determine how important we want to be in future rewards. A high value (about 1) considers long term rewards and about 0 makes the agent consider the immediate recommendation, i.e. greedy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learned value is a combination of the rewards for taking the current value in the current state and the discounted maximum rewards of the next state we will be in once we take the current action.&lt;/p&gt;

&lt;p&gt;To prevent the algorithm from learning a “fixed path” (overfitting) an Epsilon parameter ($\epsilon$) is introduced. It combines two states: the exploration and exploitation dilemma.&lt;/p&gt;

&lt;p&gt;A low epsilon will occur in episodes with more penalties. This will be the case when the algorithm is exploring, since the parameter $\epsilon$ does not influence the agent. The result is as follows:&lt;/p&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;a href=&quot;/assets/images/logbook/week78/tutorial-q-learning.gif&quot; title=&quot;Without QLearning&quot;&gt;
        &lt;img src=&quot;/assets/images/logbook/week78/tutorial-q-learning.gif&quot; alt=&quot;Without QLearning&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/logbook/week78/tutorial-q-learning_solution.gif&quot; title=&quot;Solution using QLearning.&quot;&gt;
        &lt;img src=&quot;/assets/images/logbook/week78/tutorial-q-learning_solution.gif&quot; alt=&quot;Solution using QLearning.&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;Taxi solution. Left with NO QLearning. Right with QLearning algorithm.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;With this tutorial I understood how the $\epsilon$ parameter works in this field. The algorithm would act in certain cases just like humans. If something has gone well for me, why try different combinations? With this parameter the algorithm is being forced to explore other paths or to exploit the best path.&lt;/p&gt;

&lt;p&gt;For more information visit the original article or the Jupyter notebook mentioned above.&lt;/p&gt;

&lt;h2 id=&quot;items-read&quot;&gt;Items read.&lt;/h2&gt;

&lt;p&gt;After this, I try to read other papers related to other types of Reinforcement Learning. These papers are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://karpathy.github.io/2016/05/31/rl/&quot;&gt;&lt;strong&gt;Pong from pixels&lt;/strong&gt;&lt;/a&gt;. The tutorial has been followed and analyzed in this &lt;a href=&quot;https://github.com/igarag/machine_learning/blob/master/reinforcement-learning-projects/pong/QLearning/pong_notebook.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296&quot;&gt;&lt;strong&gt;Playing CartPole through Asynchronous Advantage Actor Critic (A3C)&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://hackernoon.com/google-xs-deep-reinforcement-learning-in-robotics-using-vision-7a78e87ab171&quot;&gt;&lt;strong&gt;Google X’s Deep Reinforcement Learning in Robotics using Vision&lt;/strong&gt;&lt;/a&gt;. Presents the &lt;a href=&quot;https://arxiv.org/abs/1806.10293&quot;&gt;Qt-Opt algorithm&lt;/a&gt; is designed by combining two methods:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Large-scale Distributed Optimization&lt;/strong&gt;. Using multiple robots to train model faster, making it a large-scale distributed system.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Deep Q-learning algorithm&lt;/strong&gt;. RL technique used to learn a policy, which tells an agent which action to take under which circumstances.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="tutorial" /><category term="previous work" /><category term="weeks 7-8" /><summary type="html">Apply what you have learned to a small tutorial</summary></entry><entry><title type="html">Week 6. Summary of definitions and concepts.</title><link href="http://localhost:4000/previous%20work/week-6/" rel="alternate" type="text/html" title="Week 6. Summary of definitions and concepts." /><published>2019-09-06T00:00:00+02:00</published><updated>2019-09-06T00:00:00+02:00</updated><id>http://localhost:4000/previous%20work/week-6</id><content type="html" xml:base="http://localhost:4000/previous%20work/week-6/">&lt;p&gt;With the aim of compiling all the information and structuring it, I have been gathering from all sources the information necessary to understand and for other people, reading this blog, to understand what has been read.&lt;/p&gt;

&lt;p&gt;In a summarized way, they are explained in these points.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;In the 1950s, &lt;a href=&quot;https://en.wikipedia.org/wiki/Richard_E._Bellman&quot;&gt;Richard Bellman&lt;/a&gt; devised an approach to the problem, involving the dynamical system’s state and a value function, based on an equation, which today we call the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bellman_equation&quot;&gt;Bellman equation&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(x_0) = \max_{\{a_t\}_{t=0}^\infty}\sum_{t=0}^\infty\beta^tF(x_t, a_t)&lt;/script&gt;

&lt;p&gt;Bellman also introduced the discrete stochastic version of the problem known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;&lt;em&gt;Markov decision process&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So how does &lt;strong&gt;Reinforcement Learning&lt;/strong&gt; compare to &lt;strong&gt;other types&lt;/strong&gt; of learning? We’ll divide them into three types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt;: the goal is to learn to predict the $Y$ label, given the associated $X$ data, in such a way that the learning generalizes to unseen data beyond the training data. For the training data the $Y$ label’s been supplied by an expert. It’s like giving the answer to a student on a test.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised learning&lt;/strong&gt;: attempts to learn the structure hidden in a data set. Towards a predefined type of representation like clustering, anomaly detection or independent representation. Unsupervised learning takes no action and receives no feedback, it just operates on the $X$ data set.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; (RL), a agent must learn which action to select at each time stamp. Receiving a reward as feedback, usually sparse in nature. This feedback instead of being the correct answer is a scalar number representing the relative goodness of the sequences of action recently taken usually without a firm’s starting point for the sequence. The agent must learn the sequence that gives the highest total reward through trial and error. Also in reinforcement learning, the next state and reward functions are usually stochastic. The same action and the same state may produce different rewards and different next states for the agent. We consider reinforcement learning to be a third type of machine learning. Even though it may use supervised learning, or unsupervised learning as part of it’s method, it is a distinct type of learning with its own set of challenges and methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other terms related to reinforcement learning are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Value functions&lt;/strong&gt;: Measures the goodness of a state in the long run as calculated by an agent. Another way to more formally state this, it’s the expected long-term accumulation of reward, starting from state $s$, and following policy $\pi$. So a human analogy, the reward is kind of like an immediate pleasure or pain experienced by a person. But the value function represents a more farsighted judgment to the value of a state. So if we’re looking at the game of &lt;em&gt;tic-tac-toe&lt;/em&gt; and we’re the X player and we’re about to make our opening move and we’re evaluating different moves, this would be a move of high value because it has the best chance of winning the game. But this would be a move of low value. Value functions come in two basic flavors. There’s the straight value function, whose notation is V $\pi(s)$, where s is a state. This represents the goodness of that state, following policy pi. And then we have the Q function, which is parameterized also by an action. So Q $\pi(s,a)$, s is the state, a is the action that represents the goodness of that state, first taking action a, and then thereafter, following normal policy $\pi$ .&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;: is a mapping from a particular state to an action to take in that state. And this can be deterministic, where it’s the same action each time. Or it can be stochastic, where $70\%$ of the time, you take action one, and $30\%$ of the time, you take action two. And its notation is $\pi$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Exploration-exploitation dilemma&lt;/strong&gt;: It is one of the great dilemmas of reinforcement learning, when should the agent try and find better actions to &lt;strong&gt;explore&lt;/strong&gt; the environment and when should it &lt;strong&gt;exploit&lt;/strong&gt; the optimal action (accumulate the highest reward possible) to make progress in learning?. Some algorithms often uses the simple and greedy exploration policy where it chooses a random action. As the best option decreases over time, the agent progresses to exploitation. Let’s say we have these three doors. One on the left we tried opening and received 10€. The one in the middle we tried opening and received 5€ and the one on the right we haven’t tried yet. What should be our next action?. A greedy policy is one that always chooses the best action in a given state. The dilemma is that any successful policy will need to do a mixture of both of these. Note that this explore-exploit issue is unique to RL, it is not found in supervised or unsupervised learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time step&lt;/strong&gt;. The time step divides time into discrete steps. And each of these steps determines a cycle in the environment-agent interaction. And we usually denote this time by $t$. The environment is what defines the world that the agent interacts with, and it has a basic loop that it follows. It produces a state and a reward for the agent to sense and process. And then it accepts an action from the agent and cycles back to produce another state again. The agent learns to achieve goals by interacting with the environment. Its basic loop is it senses the state and the reward from the environment, and then selects an action to pass to the environment, and then continues that in a loop. So state represents the situation in the environment that the agent is going to make his actions based on. So an example of a discrete state is a tic-tac-toe collection of squares, where each square is either blank, has an X, or has an O on it. An example of a high-dimensional state might be the pixels in a video image from a game. And an example of continuous states could be three continuous values, temperature, pressure, and flow in an industrial controller.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Is a scalar value, a floating point number, returned by the environment when the agent selects an action. It represents the goal or a set of goals. It’s determined by the reinforcement learning problem designer himself. And its usual notation is $r_t$ (for the reward at time $t$). An action is what an agent takes on each time step. It can be discrete as of one of a fixed number of actions, like in a video game, you might go left, right, up, or down. Or it might be a continuous action. Let’s say, in a self-driving car, you might be steering the wheel at a certain angle or changing the angle of the gas pedal to feed more or less gas in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model of the environment&lt;/strong&gt;: So in a model of the environment, we basically get the transition probability to the next state and the probability of the reward going to that state. So here’s a table that represents a model. It has state-action pairs $S1$ and $A1$. And the first entry is, the next state is $S2$, and the reward is $-1$, and the probability is $0.3$. So this is a stochastic model. Not every time you’re in state $1$ action and you take action $1$ will you go to state $2$. If we look at the second line, we have state 1 and action 1, and that takes us, in this case, to state 3 with a reward of 0. And the probability for that is $0.7$. There are two kinds of methods related to models. One is called model-free methods, where you do not have a model, and that’s a pure trial-and-error learning experience. When you have a model, it’s considered to be a planning kind of learner because you typically don’t take actions in the environment. You instead just follow what would happen through the model itself to find out what your next state is and what your reward is. And you can keep doing this over and over again to find out pass and then find the value of pass. And these model-based problems come in two subclasses. One is where you’re given the model up-front, and that’s pretty rare. And the more active area of research right now is learning the model while you’re exploring.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Episodic and continuing tasks&lt;/strong&gt;: An episodic task, we have those tasks that come to a natural end, and they’re typically repeated over and over. And one example would be a &lt;em&gt;tic-tac-toe&lt;/em&gt; game, where you get a reward at the end. Another example would be the Pac-man game, where you get rewards along the way. An example of a continuing task would be controlling an air conditioner. Typically, you would be sensing and controlling at each time step, but there’s no natural endpoint. You just keep going and going.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example with &lt;em&gt;tic-tac-toe&lt;/em&gt; game:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Term&lt;/th&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Agent&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;X&lt;/em&gt; player&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Environment&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;O&lt;/em&gt; player, general rules&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;9x square occupant: &lt;em&gt;X&lt;/em&gt;, &lt;em&gt;O&lt;/em&gt;, blank.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Actions&lt;/td&gt;
      &lt;td&gt;9x place &lt;em&gt;X&lt;/em&gt; in square.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Reward&lt;/td&gt;
      &lt;td&gt;At end of the game: 1 = win, 0 = tie, -1 = loss&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Task Type&lt;/td&gt;
      &lt;td&gt;Episodic (the reward is given at the end)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Approaches to solving RL Problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Value function methods&lt;/strong&gt;: Estimate value states (or state-action pairs). Policy based on selecting actions that lead to large value states.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Direct Policy Search&lt;/strong&gt;: Model the policy itself (state &lt;img class=&quot;emoji&quot; title=&quot;:arrow_right:&quot; alt=&quot;:arrow_right:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/27a1.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt; action). Adjust model parameter in direction of greatest policy improvements.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>NachoAz</name></author><category term="deep reinforcement learning" /><category term="information" /><category term="theory." /><summary type="html">Definitions of agents, environments, states, actions, rewards, ...</summary></entry><entry><title type="html">Week 1-5. Previous Work. Soaking me in knowledge.</title><link href="http://localhost:4000/previous%20work/week-1-5/" rel="alternate" type="text/html" title="Week 1-5. Previous Work. Soaking me in knowledge." /><published>2019-08-30T00:00:00+02:00</published><updated>2019-08-30T00:00:00+02:00</updated><id>http://localhost:4000/previous%20work/week-1-5</id><content type="html" xml:base="http://localhost:4000/previous%20work/week-1-5/">&lt;h2 id=&quot;reading-documentation-state-of-the-art&quot;&gt;Reading Documentation. State of the Art.&lt;/h2&gt;

&lt;p&gt;These first weeks we use them to acquire knowledge about everything related to Reinforcement Learning.&lt;/p&gt;

&lt;p&gt;The tutors gave me a lot of information to land in this branch of machine learning so I ordered this information to gain knowledge little by little.&lt;/p&gt;

&lt;p&gt;The resources used these weeks have been:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.edx.org/es/course/reinforcement-learning-explained-3&quot;&gt;edx course on Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Reading of two papers:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;Human-level control through deep reinforcement learning&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.02298.pdf&quot;&gt;Rainbow: Combining Improvements in Deep Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As this information, without prior knowledge, was very extensive and overwhelming, I found on the &lt;a href=&quot;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&quot;&gt;OpenAi page&lt;/a&gt; information about the definitions and actors that come into play in each Reinforced Learning algorithm.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="previous work" /><category term="weeks 1-6" /><summary type="html">Reading articles, tutorials and collecting information</summary></entry><entry><title type="html">Week 0. Hello World</title><link href="http://localhost:4000/previous%20work/week-0/" rel="alternate" type="text/html" title="Week 0. Hello World" /><published>2019-07-30T00:00:00+02:00</published><updated>2019-07-30T00:00:00+02:00</updated><id>http://localhost:4000/previous%20work/week-0</id><content type="html" xml:base="http://localhost:4000/previous%20work/week-0/">&lt;p&gt;Hello, blog reader!.&lt;/p&gt;

&lt;p&gt;I’m Nacho Arranz, student of the Master in &lt;a href=&quot;https://mastervisionartificial.es&quot;&gt;Artificial Vision at Universidad Rey Juan Carlos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’m preparing for this new adventure where we will face a problem that I haven’t studied before. This will be a challenge that will grow progressively.&lt;/p&gt;

&lt;p&gt;With the post that are published every week we will be telling you the advances, problems and solutions about the project and I hope it will be nice to read it and follow it.&lt;/p&gt;

&lt;p&gt;Nothing would make me prouder than these post and the tutorials that are presented to help someone understand and learn Reinforcement Learning.&lt;/p&gt;

&lt;p&gt;The adventure begins!.&lt;/p&gt;

&lt;figure&gt;
	&lt;a href=&quot;&quot;&gt;&lt;img src=&quot;/assets/images/posts/the-adventure-begins.gif&quot; /&gt;&lt;/a&gt;
	&lt;!-- &lt;figcaption&gt;.&lt;/figcaption&gt; --&gt;
&lt;/figure&gt;

&lt;p&gt;Nacho.&lt;/p&gt;</content><author><name>NachoAz</name></author><category term="logbook" /><category term="first steps" /><category term="week 0" /><summary type="html">Introduce to myself</summary></entry></feed>